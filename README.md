# Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1dc4Q9W4KmV5FJg8GTkpKQ-Hbnx43iNWD?usp=sharing)

A re-implementation of the paper [Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability](https://arxiv.org/abs/2103.00065v3) by Cohen et al., specifically the experiment described in Appendix L.2 of the paper.

Thanks to the Honghua Dong and Tianxing Li at University of Toronto, for their [implementation](https://colab.research.google.com/drive/1yXyJIAAqFHAV_uNoW5WGBIfyp-5gpGWu?usp=sharing). I learnt a lot from it and extended the implementation to do Gradient Flow training as well.

All the plots and the code can be found in the Jupyter notebook.